# services/src/api.py
"""
FastAPI REST API for AEGIS Chat System

This module provides a minimal FastAPI interface to the AEGIS chat system,
exposing the core functionality as REST endpoints while keeping the existing
folder structure and logic intact.

Endpoints:
    POST /chat - Process a conversation through AEGIS agents
    GET /health - Health check endpoint
    GET /docs - Automatic API documentation

Dependencies:
    - fastapi
    - pydantic
    - uvicorn
"""

from fastapi import FastAPI, HTTPException, status
from fastapi.middleware.cors import CORSMiddleware
from fastapi.responses import StreamingResponse
from pydantic import BaseModel, Field
from typing import List, Dict, Any, Optional
import logging
import time
import asyncio
import json

# Import your existing modules
from .initial_setup.logging_config import configure_logging
from .initial_setup.env_config import config

# Configure logging
configure_logging()
logger = logging.getLogger(__name__)

# Create FastAPI app
app = FastAPI(
    title="AEGIS Chat API",
    description="AEGIS Intelligent Response System - AI-powered financial market data assistant",
    version="1.0.0",
    docs_url="/docs",
    redoc_url="/redoc",
)

# Add CORS middleware for financial system environment
app.add_middleware(
    CORSMiddleware,
    allow_origins=["*"],  # Configure as needed for security requirements
    allow_credentials=True,
    allow_methods=["GET", "POST"],
    allow_headers=["*"],
)


# Pydantic models for request/response
class ChatMessage(BaseModel):
    role: str = Field(..., description="Message role: 'user' or 'assistant'")
    content: str = Field(..., description="Message content")


class ChatRequest(BaseModel):
    messages: List[ChatMessage] = Field(..., description="Conversation history")
    stream: bool = Field(default=False, description="Enable streaming response")
    db_names: Optional[List[str]] = Field(
        default=None, description="List of database names to query"
    )


class ChatResponse(BaseModel):
    response: str = Field(..., description="AEGIS response")


class HealthResponse(BaseModel):
    status: str
    environment: str
    version: str


# Import chat model function after FastAPI setup to avoid circular imports
def get_chat_processor():
    """
    Lazy import to avoid circular dependencies.

    Returns:
        Callable: The async chat model function for processing conversations
    """
    try:
        from .chat_model.model import process_request_async

        return process_request_async
    except ImportError:
        logger.error(
            "Failed to import chat model. Make sure to add the async wrapper to model.py"
        )
        raise ImportError("Chat model not properly configured for async operation")


def get_streaming_chat_processor():
    """
    Lazy import for streaming chat processor.

    Returns:
        Callable: The streaming chat model function for processing conversations
    """
    try:
        from .chat_model.model import model

        return model
    except ImportError:
        logger.error("Failed to import streaming chat model")
        raise ImportError("Streaming chat model not properly configured")


async def stream_chat_response(
    conversation: List[Dict[str, str]], db_names: Optional[List[str]] = None
):
    """
    Generator function for streaming chat responses.
    Yields chunks as they are generated by the AEGIS model.
    """
    import concurrent.futures
    import queue
    import threading

    try:
        # Get the streaming chat processor
        model_func = get_streaming_chat_processor()

        # Convert to expected format
        conversation_dict = {"messages": conversation}

        # Use a queue to pass chunks between threads
        chunk_queue: queue.Queue = queue.Queue()
        exception_container = [None]

        def run_sync_generator():
            """Run the synchronous generator in a separate thread"""
            try:
                for chunk in model_func(
                    conversation_dict, debug_mode=False, db_names=db_names
                ):
                    if isinstance(chunk, str):
                        chunk_queue.put(chunk)
                chunk_queue.put(None)  # Sentinel to indicate completion
            except Exception as e:
                exception_container[0] = e
                chunk_queue.put(None)

        # Start the generator in a separate thread
        thread = threading.Thread(target=run_sync_generator)
        thread.start()

        # Stream chunks as they become available
        while True:
            # Check for chunks without blocking too long
            try:
                chunk = chunk_queue.get(timeout=0.1)
                if chunk is None:  # Sentinel value
                    break
                # Yield chunk as-is to preserve original spacing and timing
                yield chunk
                # Give control back to event loop to ensure chunk is flushed
                await asyncio.sleep(0)
            except queue.Empty:
                # Check if there's an exception
                if exception_container[0]:
                    raise exception_container[0]
                # Give control back to the event loop
                await asyncio.sleep(0.01)
                continue

        # Wait for thread to complete
        thread.join(timeout=1)

        # Check for exceptions after completion
        if exception_container[0]:
            raise exception_container[0]

    except Exception as e:
        logger.error(f"Streaming error: {str(e)}", exc_info=True)
        yield f"Error: {str(e)}"


@app.post("/chat")
async def chat_endpoint(request: ChatRequest):
    """
    Process a conversation through the AEGIS system.

    This endpoint accepts a conversation history and routes it through the
    appropriate AEGIS agents to generate a response.

    If stream=true, returns Server-Sent Events stream.
    If stream=false, returns JSON response.
    """
    try:
        logger.info(
            f"Received chat request with {len(request.messages)} messages, stream={request.stream}"
        )

        # Convert Pydantic models to dict format expected by existing code
        conversation = [
            {"role": msg.role, "content": msg.content} for msg in request.messages
        ]

        if request.stream:
            # Return streaming response
            logger.info("Returning streaming response")
            return StreamingResponse(
                stream_chat_response(conversation, request.db_names),
                media_type="text/plain",
                headers={
                    "Cache-Control": "no-cache",
                    "Connection": "keep-alive",
                },
            )
        else:
            # Return complete response
            logger.info("Returning complete response")
            process_request_async = get_chat_processor()
            result = await process_request_async(
                conversation, stream=False, db_names=request.db_names
            )

            logger.info("Chat request processed successfully")
            return ChatResponse(response=result.get("response", ""))

    except Exception as e:
        logger.error(f"Chat endpoint error: {str(e)}", exc_info=True)
        raise HTTPException(
            status_code=status.HTTP_500_INTERNAL_SERVER_ERROR,
            detail=f"Internal server error: {str(e)}",
        )


@app.get("/health", response_model=HealthResponse)
async def health_check():
    """
    Health check endpoint to verify the API is running and properly configured.
    """
    try:
        # Validate configuration
        config.validate()

        return HealthResponse(
            status="healthy", environment=config.ENVIRONMENT, version="1.0.0"
        )
    except Exception as e:
        logger.error(f"Health check failed: {str(e)}")
        raise HTTPException(
            status_code=status.HTTP_503_SERVICE_UNAVAILABLE,
            detail=f"Service unhealthy: {str(e)}",
        )


@app.get("/")
async def root():
    """
    Root endpoint with basic information about the API.
    """
    return {
        "message": "AEGIS Chat API",
        "docs": "/docs",
        "health": "/health",
        "version": "1.0.0",
    }


# Startup event to validate configuration
@app.on_event("startup")
async def startup_event():
    """
    Perform startup validation and initialization.
    """
    logger.info("Starting AEGIS Chat API...")

    try:
        # Validate environment configuration
        if not config.validate():
            raise ValueError("Configuration validation failed")

        logger.info(
            f"AEGIS Chat API started successfully in {config.ENVIRONMENT} environment"
        )

    except Exception as e:
        logger.error(f"Startup failed: {str(e)}")
        raise


# Shutdown event
@app.on_event("shutdown")
async def shutdown_event():
    """
    Cleanup on shutdown.
    """
    logger.info("Shutting down AEGIS Chat API...")


if __name__ == "__main__":
    import uvicorn

    # Development server
    uvicorn.run(
        "services.src.api:app", host="0.0.0.0", port=8000, reload=True, log_level="info"
    )
